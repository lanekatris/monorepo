{
  "plugin_type": "extractors",
  "name": "tap-s3",
  "namespace": "tap_airbyte",
  "variant": "airbyte",
  "label": "S3",
  "docs": "https://hub.meltano.com/extractors/tap-s3--airbyte",
  "repo": "https://github.com/airbytehq/airbyte/tree/master/airbyte-integrations/connectors/source-s3",
  "pip_url": "git+https://github.com/MeltanoLabs/tap-airbyte-wrapper.git",
  "executable": "tap-airbyte",
  "description": "AWS S3",
  "logo_url": "https://hub.meltano.com/assets/logos/extractors/s3-csv.png",
  "capabilities": [
    "catalog",
    "state",
    "discover",
    "about",
    "stream-maps",
    "schema-flattening"
  ],
  "settings_group_validation": [
    [
      "connector_config.provider.bucket",
      "connector_config.path_pattern",
      "airbyte_spec.image",
      "connector_config.dataset"
    ]
  ],
  "settings": [
    {
      "name": "airbyte_spec.image",
      "kind": "string",
      "value": "airbyte/source-s3",
      "label": "Airbyte Spec Image",
      "description": "Airbyte image to run"
    },
    {
      "name": "airbyte_spec.tag",
      "kind": "string",
      "value": "latest",
      "label": "Airbyte Spec Tag",
      "description": "Airbyte image tag"
    },
    {
      "name": "docker_mounts",
      "kind": "array",
      "label": "Docker Mounts",
      "description": "Docker mounts to make available to the Airbyte container. Expects a list of maps containing source, target, and type as is documented in the docker --mount documentation"
    },
    {
      "name": "airbyte_config.dataset",
      "kind": "string",
      "label": "Airbyte Config Dataset",
      "description": "The name of the stream you would like this source to output. Can contain letters, numbers, or underscores."
    },
    {
      "name": "airbyte_config.path_pattern",
      "kind": "string",
      "label": "Airbyte Config Path Pattern",
      "description": "A regular expression which tells the connector which files to replicate. All files which match this pattern will be replicated. Use | to separate multiple patterns. See <a href=\"https://facelessuser.github.io/wcmatch/glob/\" target=\"_blank\">this page</a> to understand pattern syntax (GLOBSTAR and SPLIT flags are enabled). Use pattern <strong>**</strong> to pick up all files."
    },
    {
      "name": "airbyte_config.format.filetype",
      "kind": "string",
      "label": "Airbyte Config Format Filetype",
      "description": "csv, parquet, avro, jsonl"
    },
    {
      "name": "airbyte_config.format.delimiter",
      "kind": "string",
      "label": "Airbyte Config Format Delimiter",
      "description": "The character delimiting individual cells in the CSV data. This may only be a 1-character string. For tab-delimited data enter '\\t'."
    },
    {
      "name": "airbyte_config.format.infer_datatypes",
      "kind": "boolean",
      "label": "Airbyte Config Format Infer Datatypes",
      "description": "Configures whether a schema for the source should be inferred from the current data or not. If set to false and a custom schema is set, then the manually enforced schema is used. If a schema is not manually set, and this is set to false, then all fields will be read as strings"
    },
    {
      "name": "airbyte_config.format.quote_char",
      "kind": "string",
      "label": "Airbyte Config Format Quote Char",
      "description": "The character used for quoting CSV values. To disallow quoting, make this field blank."
    },
    {
      "name": "airbyte_config.format.escape_char",
      "kind": "string",
      "label": "Airbyte Config Format Escape Char",
      "description": "The character used for escaping special characters. To disallow escaping, leave this field blank."
    },
    {
      "name": "airbyte_config.format.encoding",
      "kind": "string",
      "label": "Airbyte Config Format Encoding",
      "description": "The character encoding of the CSV data. Leave blank to default to <strong>UTF8</strong>. See <a href=\"https://docs.python.org/3/library/codecs.html#standard-encodings\" target=\"_blank\">list of python encodings</a> for allowable options."
    },
    {
      "name": "airbyte_config.format.double_quote",
      "kind": "boolean",
      "label": "Airbyte Config Format Double Quote",
      "description": "Whether two quotes in a quoted CSV value denote a single quote in the data."
    },
    {
      "name": "airbyte_config.format.newlines_in_values",
      "kind": "boolean",
      "label": "Airbyte Config Format Newlines In Values",
      "description": "Whether newline characters are allowed in CSV values. Turning this on may affect performance. Leave blank to default to False., Whether newline characters are allowed in JSON values. Turning this on may affect performance. Leave blank to default to False."
    },
    {
      "name": "airbyte_config.format.additional_reader_options",
      "kind": "string",
      "label": "Airbyte Config Format Additional Reader Options",
      "description": "Optionally add a valid JSON string here to provide additional options to the csv reader. Mappings must correspond to options <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions\" target=\"_blank\">detailed here</a>. 'column_types' is used internally to handle schema so overriding that would likely cause problems."
    },
    {
      "name": "airbyte_config.format.advanced_options",
      "kind": "string",
      "label": "Airbyte Config Format Advanced Options",
      "description": "Optionally add a valid JSON string here to provide additional <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions\" target=\"_blank\">Pyarrow ReadOptions</a>. Specify 'column_names' here if your CSV doesn't have header, or if you want to use custom column names. 'block_size' and 'encoding' are already used above, specify them again here will override the values above."
    },
    {
      "name": "airbyte_config.format.block_size",
      "kind": "integer",
      "label": "Airbyte Config Format Block Size",
      "description": "The chunk size in bytes to process at a time in memory from each file. If your data is particularly wide and failing during schema detection, increasing this should solve it. Beware of raising this too high as you could hit OOM errors., The chunk size in bytes to process at a time in memory from each file. If your data is particularly wide and failing during schema detection, increasing this should solve it. Beware of raising this too high as you could hit OOM errors."
    },
    {
      "name": "airbyte_config.format.columns",
      "kind": "array",
      "label": "Airbyte Config Format Columns",
      "description": "If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns."
    },
    {
      "name": "airbyte_config.format.batch_size",
      "kind": "integer",
      "label": "Airbyte Config Format Batch Size",
      "description": "Maximum number of records per batch read from the input files. Batches may be smaller if there aren\u2019t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide."
    },
    {
      "name": "airbyte_config.format.buffer_size",
      "kind": "integer",
      "label": "Airbyte Config Format Buffer Size",
      "description": "Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide."
    },
    {
      "name": "airbyte_config.format.unexpected_field_behavior",
      "kind": "string",
      "label": "Airbyte Config Format Unexpected Field Behavior",
      "description": "How JSON fields outside of explicit_schema (if given) are treated. Check <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html\" target=\"_blank\">PyArrow documentation</a> for details"
    },
    {
      "name": "airbyte_config.schema",
      "kind": "string",
      "label": "Airbyte Config Schema",
      "description": "Optionally provide a schema to enforce, as a valid JSON string. Ensure this is a mapping of <strong>{ \"column\" : \"type\" }</strong>, where types are valid <a href=\"https://json-schema.org/understanding-json-schema/reference/type.html\" target=\"_blank\">JSON Schema datatypes</a>. Leave as {} to auto-infer the schema."
    },
    {
      "name": "airbyte_config.provider.bucket",
      "kind": "password",
      "label": "Airbyte Config Provider Bucket",
      "description": "Name of the S3 bucket where the file(s) exist."
    },
    {
      "name": "airbyte_config.provider.aws_access_key_id",
      "kind": "password",
      "label": "Airbyte Config Provider AWS Access Key Id",
      "description": "In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary."
    },
    {
      "name": "airbyte_config.provider.aws_secret_access_key",
      "kind": "password",
      "label": "Airbyte Config Provider AWS Secret Access Key",
      "description": "In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary."
    },
    {
      "name": "airbyte_config.provider.path_prefix",
      "kind": "password",
      "label": "Airbyte Config Provider Path Prefix",
      "description": "By providing a path-like prefix (e.g. myFolder/thisTable/) under which all the relevant files sit, we can optimize finding these in S3. This is optional but recommended if your bucket contains many folders/files which you don't need to replicate."
    },
    {
      "name": "airbyte_config.provider.endpoint",
      "kind": "password",
      "label": "Airbyte Config Provider Endpoint",
      "description": "Endpoint to an S3 compatible service. Leave empty to use AWS."
    },
    {
      "name": "stream_maps",
      "kind": "object",
      "label": "Stream Maps",
      "description": "Config object for stream maps capability. For more information check out [Stream Maps](https://sdk.meltano.com/en/latest/stream_maps.html)."
    },
    {
      "name": "stream_map_config",
      "kind": "object",
      "label": "Stream Map Config",
      "description": "User-defined config values to be used within map expressions."
    },
    {
      "name": "flattening_enabled",
      "kind": "boolean",
      "label": "Flattening Enabled",
      "description": "'True' to enable schema flattening and automatically expand nested properties."
    },
    {
      "name": "flattening_max_depth",
      "kind": "integer",
      "label": "Flattening Max Depth",
      "description": "The max depth to flatten schemas."
    }
  ]
}